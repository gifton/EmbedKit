program(1.0)
[buildInfo = dict<tensor<string, []>, tensor<string, []>>({{"coremlc-component-MIL", "3500.14.1"}, {"coremlc-version", "3500.32.1"}, {"coremltools-component-torch", "2.7.0"}, {"coremltools-source-dialect", "TorchScript"}, {"coremltools-version", "8.3.0"}})]
{
    func main<ios15>(tensor<int32, [1, 512]> attention_mask, tensor<int32, [1, 512]> input_ids, tensor<int32, [1, 512]> token_type_ids) {
            tensor<int32, []> inputs_embeds_axis_0 = const()[name = tensor<string, []>("inputs_embeds_axis_0"), val = tensor<int32, []>(0)];
            tensor<fp16, [30522, 384]> model_embeddings_word_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_word_embeddings_weight_to_fp16"), val = tensor<fp16, [30522, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64)))];
            tensor<fp16, [1, 512, 384]> inputs_embeds_cast_fp16 = gather(axis = inputs_embeds_axis_0, indices = input_ids, x = model_embeddings_word_embeddings_weight_to_fp16)[name = tensor<string, []>("inputs_embeds_cast_fp16")];
            tensor<int32, []> token_type_embeddings_1_axis_0 = const()[name = tensor<string, []>("token_type_embeddings_1_axis_0"), val = tensor<int32, []>(0)];
            tensor<fp16, [2, 384]> model_embeddings_token_type_embeddings_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_token_type_embeddings_weight_to_fp16"), val = tensor<fp16, [2, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23441024)))];
            tensor<fp16, [1, 512, 384]> token_type_embeddings_1_cast_fp16 = gather(axis = token_type_embeddings_1_axis_0, indices = token_type_ids, x = model_embeddings_token_type_embeddings_weight_to_fp16)[name = tensor<string, []>("token_type_embeddings_1_cast_fp16")];
            tensor<fp16, [1, 512, 384]> embeddings_1_cast_fp16 = add(x = inputs_embeds_cast_fp16, y = token_type_embeddings_1_cast_fp16)[name = tensor<string, []>("embeddings_1_cast_fp16")];
            tensor<fp16, [1, 512, 384]> position_embeddings_1_to_fp16 = const()[name = tensor<string, []>("position_embeddings_1_to_fp16"), val = tensor<fp16, [1, 512, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23442624)))];
            tensor<fp16, [1, 512, 384]> input_3_cast_fp16 = add(x = embeddings_1_cast_fp16, y = position_embeddings_1_to_fp16)[name = tensor<string, []>("input_3_cast_fp16")];
            tensor<int32, [1]> input_5_axes_0 = const()[name = tensor<string, []>("input_5_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_embeddings_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_embeddings_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23835904)))];
            tensor<fp16, [384]> model_embeddings_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_embeddings_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23836736)))];
            tensor<fp16, []> var_21_to_fp16 = const()[name = tensor<string, []>("op_21_to_fp16"), val = tensor<fp16, []>(0x1p-24)];
            tensor<fp16, [1, 512, 384]> input_5_cast_fp16 = layer_norm(axes = input_5_axes_0, beta = model_embeddings_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_embeddings_LayerNorm_weight_to_fp16, x = input_3_cast_fp16)[name = tensor<string, []>("input_5_cast_fp16")];
            tensor<int32, [1]> var_56_axes_0 = const()[name = tensor<string, []>("op_56_axes_0"), val = tensor<int32, [1]>([1])];
            tensor<int32, [1, 1, 512]> var_56 = expand_dims(axes = var_56_axes_0, x = attention_mask)[name = tensor<string, []>("op_56")];
            tensor<int32, [1]> var_57_axes_0 = const()[name = tensor<string, []>("op_57_axes_0"), val = tensor<int32, [1]>([2])];
            tensor<int32, [1, 1, 1, 512]> var_57 = expand_dims(axes = var_57_axes_0, x = var_56)[name = tensor<string, []>("op_57")];
            tensor<int32, [4]> var_60_reps_0 = const()[name = tensor<string, []>("op_60_reps_0"), val = tensor<int32, [4]>([1, 1, 512, 1])];
            tensor<int32, [1, 1, 512, 512]> var_60 = tile(reps = var_60_reps_0, x = var_57)[name = tensor<string, []>("op_60")];
            tensor<fp16, []> var_11_to_fp16 = const()[name = tensor<string, []>("op_11_to_fp16"), val = tensor<fp16, []>(0x1p+0)];
            tensor<string, []> cast_3_to_fp16_dtype_0 = const()[name = tensor<string, []>("cast_3_to_fp16_dtype_0"), val = tensor<string, []>("fp16")];
            tensor<fp16, [1, 1, 512, 512]> var_60_to_fp16 = cast(dtype = cast_3_to_fp16_dtype_0, x = var_60)[name = tensor<string, []>("cast_55")];
            tensor<fp16, [1, 1, 512, 512]> inverted_mask_cast_fp16 = sub(x = var_11_to_fp16, y = var_60_to_fp16)[name = tensor<string, []>("inverted_mask_cast_fp16")];
            tensor<string, []> cast_4_dtype_0 = const()[name = tensor<string, []>("cast_4_dtype_0"), val = tensor<string, []>("bool")];
            tensor<fp16, []> var_9_to_fp16 = const()[name = tensor<string, []>("op_9_to_fp16"), val = tensor<fp16, []>(-inf)];
            tensor<bool, [1, 1, 512, 512]> inverted_mask_cast_fp16_to_bool = cast(dtype = cast_4_dtype_0, x = inverted_mask_cast_fp16)[name = tensor<string, []>("cast_54")];
            tensor<fp16, [1, 1, 512, 512]> attention_mask_cast_fp16 = select(a = var_9_to_fp16, b = inverted_mask_cast_fp16, cond = inverted_mask_cast_fp16_to_bool)[name = tensor<string, []>("attention_mask_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(23837568)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24132544)))];
            tensor<fp16, [1, 512, 384]> linear_0_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_query_bias_to_fp16, weight = model_encoder_layer_0_attention_self_query_weight_to_fp16, x = input_5_cast_fp16)[name = tensor<string, []>("linear_0_cast_fp16")];
            tensor<int32, [4]> var_104 = const()[name = tensor<string, []>("op_104"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_3_cast_fp16 = reshape(shape = var_104, x = linear_0_cast_fp16)[name = tensor<string, []>("x_3_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24133376)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24428352)))];
            tensor<fp16, [1, 512, 384]> linear_1_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_key_bias_to_fp16, weight = model_encoder_layer_0_attention_self_key_weight_to_fp16, x = input_5_cast_fp16)[name = tensor<string, []>("linear_1_cast_fp16")];
            tensor<int32, [4]> var_113 = const()[name = tensor<string, []>("op_113"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_7_cast_fp16 = reshape(shape = var_113, x = linear_1_cast_fp16)[name = tensor<string, []>("x_7_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24429184)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24724160)))];
            tensor<fp16, [1, 512, 384]> linear_2_cast_fp16 = linear(bias = model_encoder_layer_0_attention_self_value_bias_to_fp16, weight = model_encoder_layer_0_attention_self_value_weight_to_fp16, x = input_5_cast_fp16)[name = tensor<string, []>("linear_2_cast_fp16")];
            tensor<int32, [4]> var_122 = const()[name = tensor<string, []>("op_122"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_11_cast_fp16 = reshape(shape = var_122, x = linear_2_cast_fp16)[name = tensor<string, []>("x_11_cast_fp16")];
            tensor<int32, [4]> var_124 = const()[name = tensor<string, []>("op_124"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_0_y_0_to_fp16 = const()[name = tensor<string, []>("mul_0_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_0_cast_fp16 = mul(x = x_3_cast_fp16, y = mul_0_y_0_to_fp16)[name = tensor<string, []>("mul_0_cast_fp16")];
            tensor<bool, []> matmul_0_transpose_y_0 = const()[name = tensor<string, []>("matmul_0_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_0_transpose_x_0 = const()[name = tensor<string, []>("matmul_0_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_48_perm_0 = const()[name = tensor<string, []>("transpose_48_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_49_perm_0 = const()[name = tensor<string, []>("transpose_49_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_49 = transpose(perm = transpose_49_perm_0, x = x_7_cast_fp16)[name = tensor<string, []>("transpose_117")];
            tensor<fp16, [1, 12, 512, 32]> transpose_48 = transpose(perm = transpose_48_perm_0, x = mul_0_cast_fp16)[name = tensor<string, []>("transpose_118")];
            tensor<fp16, [1, 12, 512, 512]> matmul_0_cast_fp16 = matmul(transpose_x = matmul_0_transpose_x_0, transpose_y = matmul_0_transpose_y_0, x = transpose_48, y = transpose_49)[name = tensor<string, []>("matmul_0_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_0_cast_fp16 = add(x = matmul_0_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_0_cast_fp16")];
            tensor<int32, []> softmax_0_axis_0 = const()[name = tensor<string, []>("softmax_0_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_0_cast_fp16 = softmax(axis = softmax_0_axis_0, x = add_0_cast_fp16)[name = tensor<string, []>("softmax_0_cast_fp16")];
            tensor<bool, []> attn_output_1_transpose_x_0 = const()[name = tensor<string, []>("attn_output_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_1_transpose_y_0 = const()[name = tensor<string, []>("attn_output_1_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_1_cast_fp16 = transpose(perm = var_124, x = x_11_cast_fp16)[name = tensor<string, []>("transpose_119")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_1_cast_fp16 = matmul(transpose_x = attn_output_1_transpose_x_0, transpose_y = attn_output_1_transpose_y_0, x = softmax_0_cast_fp16, y = value_layer_1_cast_fp16)[name = tensor<string, []>("attn_output_1_cast_fp16")];
            tensor<int32, [4]> attn_output_3_perm_0 = const()[name = tensor<string, []>("attn_output_3_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_128 = const()[name = tensor<string, []>("op_128"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_3_cast_fp16 = transpose(perm = attn_output_3_perm_0, x = attn_output_1_cast_fp16)[name = tensor<string, []>("transpose_116")];
            tensor<fp16, [1, 512, 384]> input_7_cast_fp16 = reshape(shape = var_128, x = attn_output_3_cast_fp16)[name = tensor<string, []>("input_7_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_0_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(24724992)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25019968)))];
            tensor<fp16, [1, 512, 384]> linear_3_cast_fp16 = linear(bias = model_encoder_layer_0_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_0_attention_output_dense_weight_to_fp16, x = input_7_cast_fp16)[name = tensor<string, []>("linear_3_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_11_cast_fp16 = add(x = linear_3_cast_fp16, y = input_5_cast_fp16)[name = tensor<string, []>("input_11_cast_fp16")];
            tensor<int32, [1]> input_13_axes_0 = const()[name = tensor<string, []>("input_13_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25020800)))];
            tensor<fp16, [384]> model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25021632)))];
            tensor<fp16, [1, 512, 384]> input_13_cast_fp16 = layer_norm(axes = input_13_axes_0, beta = model_encoder_layer_0_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_0_attention_output_LayerNorm_weight_to_fp16, x = input_11_cast_fp16)[name = tensor<string, []>("input_13_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_0_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(25022464)))];
            tensor<fp16, [1536]> model_encoder_layer_0_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26202176)))];
            tensor<fp16, [1, 512, 1536]> linear_4_cast_fp16 = linear(bias = model_encoder_layer_0_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_0_intermediate_dense_weight_to_fp16, x = input_13_cast_fp16)[name = tensor<string, []>("linear_4_cast_fp16")];
            tensor<string, []> input_17_mode_0 = const()[name = tensor<string, []>("input_17_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_17_cast_fp16 = gelu(mode = input_17_mode_0, x = linear_4_cast_fp16)[name = tensor<string, []>("input_17_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_0_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(26205312)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27385024)))];
            tensor<fp16, [1, 512, 384]> linear_5_cast_fp16 = linear(bias = model_encoder_layer_0_output_dense_bias_to_fp16, weight = model_encoder_layer_0_output_dense_weight_to_fp16, x = input_17_cast_fp16)[name = tensor<string, []>("linear_5_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_21_cast_fp16 = add(x = linear_5_cast_fp16, y = input_13_cast_fp16)[name = tensor<string, []>("input_21_cast_fp16")];
            tensor<int32, [1]> hidden_states_7_axes_0 = const()[name = tensor<string, []>("hidden_states_7_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27385856)))];
            tensor<fp16, [384]> model_encoder_layer_0_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_0_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27386688)))];
            tensor<fp16, [1, 512, 384]> hidden_states_7_cast_fp16 = layer_norm(axes = hidden_states_7_axes_0, beta = model_encoder_layer_0_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_0_output_LayerNorm_weight_to_fp16, x = input_21_cast_fp16)[name = tensor<string, []>("hidden_states_7_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27387520)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27682496)))];
            tensor<fp16, [1, 512, 384]> linear_6_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_query_bias_to_fp16, weight = model_encoder_layer_1_attention_self_query_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_6_cast_fp16")];
            tensor<int32, [4]> var_172 = const()[name = tensor<string, []>("op_172"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_15_cast_fp16 = reshape(shape = var_172, x = linear_6_cast_fp16)[name = tensor<string, []>("x_15_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27683328)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27978304)))];
            tensor<fp16, [1, 512, 384]> linear_7_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_key_bias_to_fp16, weight = model_encoder_layer_1_attention_self_key_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_7_cast_fp16")];
            tensor<int32, [4]> var_181 = const()[name = tensor<string, []>("op_181"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_19_cast_fp16 = reshape(shape = var_181, x = linear_7_cast_fp16)[name = tensor<string, []>("x_19_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(27979136)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28274112)))];
            tensor<fp16, [1, 512, 384]> linear_8_cast_fp16 = linear(bias = model_encoder_layer_1_attention_self_value_bias_to_fp16, weight = model_encoder_layer_1_attention_self_value_weight_to_fp16, x = hidden_states_7_cast_fp16)[name = tensor<string, []>("linear_8_cast_fp16")];
            tensor<int32, [4]> var_190 = const()[name = tensor<string, []>("op_190"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_23_cast_fp16 = reshape(shape = var_190, x = linear_8_cast_fp16)[name = tensor<string, []>("x_23_cast_fp16")];
            tensor<int32, [4]> var_192 = const()[name = tensor<string, []>("op_192"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_1_y_0_to_fp16 = const()[name = tensor<string, []>("mul_1_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_1_cast_fp16 = mul(x = x_15_cast_fp16, y = mul_1_y_0_to_fp16)[name = tensor<string, []>("mul_1_cast_fp16")];
            tensor<bool, []> matmul_1_transpose_y_0 = const()[name = tensor<string, []>("matmul_1_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_1_transpose_x_0 = const()[name = tensor<string, []>("matmul_1_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_50_perm_0 = const()[name = tensor<string, []>("transpose_50_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_51_perm_0 = const()[name = tensor<string, []>("transpose_51_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_51 = transpose(perm = transpose_51_perm_0, x = x_19_cast_fp16)[name = tensor<string, []>("transpose_113")];
            tensor<fp16, [1, 12, 512, 32]> transpose_50 = transpose(perm = transpose_50_perm_0, x = mul_1_cast_fp16)[name = tensor<string, []>("transpose_114")];
            tensor<fp16, [1, 12, 512, 512]> matmul_1_cast_fp16 = matmul(transpose_x = matmul_1_transpose_x_0, transpose_y = matmul_1_transpose_y_0, x = transpose_50, y = transpose_51)[name = tensor<string, []>("matmul_1_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_1_cast_fp16 = add(x = matmul_1_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_1_cast_fp16")];
            tensor<int32, []> softmax_1_axis_0 = const()[name = tensor<string, []>("softmax_1_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_1_cast_fp16 = softmax(axis = softmax_1_axis_0, x = add_1_cast_fp16)[name = tensor<string, []>("softmax_1_cast_fp16")];
            tensor<bool, []> attn_output_5_transpose_x_0 = const()[name = tensor<string, []>("attn_output_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_5_transpose_y_0 = const()[name = tensor<string, []>("attn_output_5_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_3_cast_fp16 = transpose(perm = var_192, x = x_23_cast_fp16)[name = tensor<string, []>("transpose_115")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_5_cast_fp16 = matmul(transpose_x = attn_output_5_transpose_x_0, transpose_y = attn_output_5_transpose_y_0, x = softmax_1_cast_fp16, y = value_layer_3_cast_fp16)[name = tensor<string, []>("attn_output_5_cast_fp16")];
            tensor<int32, [4]> attn_output_7_perm_0 = const()[name = tensor<string, []>("attn_output_7_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_196 = const()[name = tensor<string, []>("op_196"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_7_cast_fp16 = transpose(perm = attn_output_7_perm_0, x = attn_output_5_cast_fp16)[name = tensor<string, []>("transpose_112")];
            tensor<fp16, [1, 512, 384]> input_23_cast_fp16 = reshape(shape = var_196, x = attn_output_7_cast_fp16)[name = tensor<string, []>("input_23_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_1_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28274944)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28569920)))];
            tensor<fp16, [1, 512, 384]> linear_9_cast_fp16 = linear(bias = model_encoder_layer_1_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_1_attention_output_dense_weight_to_fp16, x = input_23_cast_fp16)[name = tensor<string, []>("linear_9_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_27_cast_fp16 = add(x = linear_9_cast_fp16, y = hidden_states_7_cast_fp16)[name = tensor<string, []>("input_27_cast_fp16")];
            tensor<int32, [1]> input_29_axes_0 = const()[name = tensor<string, []>("input_29_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28570752)))];
            tensor<fp16, [384]> model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28571584)))];
            tensor<fp16, [1, 512, 384]> input_29_cast_fp16 = layer_norm(axes = input_29_axes_0, beta = model_encoder_layer_1_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_1_attention_output_LayerNorm_weight_to_fp16, x = input_27_cast_fp16)[name = tensor<string, []>("input_29_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_1_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(28572416)))];
            tensor<fp16, [1536]> model_encoder_layer_1_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29752128)))];
            tensor<fp16, [1, 512, 1536]> linear_10_cast_fp16 = linear(bias = model_encoder_layer_1_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_1_intermediate_dense_weight_to_fp16, x = input_29_cast_fp16)[name = tensor<string, []>("linear_10_cast_fp16")];
            tensor<string, []> input_33_mode_0 = const()[name = tensor<string, []>("input_33_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_33_cast_fp16 = gelu(mode = input_33_mode_0, x = linear_10_cast_fp16)[name = tensor<string, []>("input_33_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_1_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(29755264)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30934976)))];
            tensor<fp16, [1, 512, 384]> linear_11_cast_fp16 = linear(bias = model_encoder_layer_1_output_dense_bias_to_fp16, weight = model_encoder_layer_1_output_dense_weight_to_fp16, x = input_33_cast_fp16)[name = tensor<string, []>("linear_11_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_37_cast_fp16 = add(x = linear_11_cast_fp16, y = input_29_cast_fp16)[name = tensor<string, []>("input_37_cast_fp16")];
            tensor<int32, [1]> hidden_states_13_axes_0 = const()[name = tensor<string, []>("hidden_states_13_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30935808)))];
            tensor<fp16, [384]> model_encoder_layer_1_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_1_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30936640)))];
            tensor<fp16, [1, 512, 384]> hidden_states_13_cast_fp16 = layer_norm(axes = hidden_states_13_axes_0, beta = model_encoder_layer_1_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_1_output_LayerNorm_weight_to_fp16, x = input_37_cast_fp16)[name = tensor<string, []>("hidden_states_13_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(30937472)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31232448)))];
            tensor<fp16, [1, 512, 384]> linear_12_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_query_bias_to_fp16, weight = model_encoder_layer_2_attention_self_query_weight_to_fp16, x = hidden_states_13_cast_fp16)[name = tensor<string, []>("linear_12_cast_fp16")];
            tensor<int32, [4]> var_240 = const()[name = tensor<string, []>("op_240"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_27_cast_fp16 = reshape(shape = var_240, x = linear_12_cast_fp16)[name = tensor<string, []>("x_27_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31233280)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31528256)))];
            tensor<fp16, [1, 512, 384]> linear_13_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_key_bias_to_fp16, weight = model_encoder_layer_2_attention_self_key_weight_to_fp16, x = hidden_states_13_cast_fp16)[name = tensor<string, []>("linear_13_cast_fp16")];
            tensor<int32, [4]> var_249 = const()[name = tensor<string, []>("op_249"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_31_cast_fp16 = reshape(shape = var_249, x = linear_13_cast_fp16)[name = tensor<string, []>("x_31_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31529088)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31824064)))];
            tensor<fp16, [1, 512, 384]> linear_14_cast_fp16 = linear(bias = model_encoder_layer_2_attention_self_value_bias_to_fp16, weight = model_encoder_layer_2_attention_self_value_weight_to_fp16, x = hidden_states_13_cast_fp16)[name = tensor<string, []>("linear_14_cast_fp16")];
            tensor<int32, [4]> var_258 = const()[name = tensor<string, []>("op_258"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_35_cast_fp16 = reshape(shape = var_258, x = linear_14_cast_fp16)[name = tensor<string, []>("x_35_cast_fp16")];
            tensor<int32, [4]> var_260 = const()[name = tensor<string, []>("op_260"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_2_y_0_to_fp16 = const()[name = tensor<string, []>("mul_2_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_2_cast_fp16 = mul(x = x_27_cast_fp16, y = mul_2_y_0_to_fp16)[name = tensor<string, []>("mul_2_cast_fp16")];
            tensor<bool, []> matmul_2_transpose_y_0 = const()[name = tensor<string, []>("matmul_2_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_2_transpose_x_0 = const()[name = tensor<string, []>("matmul_2_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_52_perm_0 = const()[name = tensor<string, []>("transpose_52_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_53_perm_0 = const()[name = tensor<string, []>("transpose_53_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_53 = transpose(perm = transpose_53_perm_0, x = x_31_cast_fp16)[name = tensor<string, []>("transpose_109")];
            tensor<fp16, [1, 12, 512, 32]> transpose_52 = transpose(perm = transpose_52_perm_0, x = mul_2_cast_fp16)[name = tensor<string, []>("transpose_110")];
            tensor<fp16, [1, 12, 512, 512]> matmul_2_cast_fp16 = matmul(transpose_x = matmul_2_transpose_x_0, transpose_y = matmul_2_transpose_y_0, x = transpose_52, y = transpose_53)[name = tensor<string, []>("matmul_2_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_2_cast_fp16 = add(x = matmul_2_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_2_cast_fp16")];
            tensor<int32, []> softmax_2_axis_0 = const()[name = tensor<string, []>("softmax_2_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_2_cast_fp16 = softmax(axis = softmax_2_axis_0, x = add_2_cast_fp16)[name = tensor<string, []>("softmax_2_cast_fp16")];
            tensor<bool, []> attn_output_9_transpose_x_0 = const()[name = tensor<string, []>("attn_output_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_9_transpose_y_0 = const()[name = tensor<string, []>("attn_output_9_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_5_cast_fp16 = transpose(perm = var_260, x = x_35_cast_fp16)[name = tensor<string, []>("transpose_111")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_9_cast_fp16 = matmul(transpose_x = attn_output_9_transpose_x_0, transpose_y = attn_output_9_transpose_y_0, x = softmax_2_cast_fp16, y = value_layer_5_cast_fp16)[name = tensor<string, []>("attn_output_9_cast_fp16")];
            tensor<int32, [4]> attn_output_11_perm_0 = const()[name = tensor<string, []>("attn_output_11_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_264 = const()[name = tensor<string, []>("op_264"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_11_cast_fp16 = transpose(perm = attn_output_11_perm_0, x = attn_output_9_cast_fp16)[name = tensor<string, []>("transpose_108")];
            tensor<fp16, [1, 512, 384]> input_39_cast_fp16 = reshape(shape = var_264, x = attn_output_11_cast_fp16)[name = tensor<string, []>("input_39_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_2_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(31824896)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32119872)))];
            tensor<fp16, [1, 512, 384]> linear_15_cast_fp16 = linear(bias = model_encoder_layer_2_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_2_attention_output_dense_weight_to_fp16, x = input_39_cast_fp16)[name = tensor<string, []>("linear_15_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_43_cast_fp16 = add(x = linear_15_cast_fp16, y = hidden_states_13_cast_fp16)[name = tensor<string, []>("input_43_cast_fp16")];
            tensor<int32, [1]> input_45_axes_0 = const()[name = tensor<string, []>("input_45_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32120704)))];
            tensor<fp16, [384]> model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32121536)))];
            tensor<fp16, [1, 512, 384]> input_45_cast_fp16 = layer_norm(axes = input_45_axes_0, beta = model_encoder_layer_2_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_2_attention_output_LayerNorm_weight_to_fp16, x = input_43_cast_fp16)[name = tensor<string, []>("input_45_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_2_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(32122368)))];
            tensor<fp16, [1536]> model_encoder_layer_2_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33302080)))];
            tensor<fp16, [1, 512, 1536]> linear_16_cast_fp16 = linear(bias = model_encoder_layer_2_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_2_intermediate_dense_weight_to_fp16, x = input_45_cast_fp16)[name = tensor<string, []>("linear_16_cast_fp16")];
            tensor<string, []> input_49_mode_0 = const()[name = tensor<string, []>("input_49_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_49_cast_fp16 = gelu(mode = input_49_mode_0, x = linear_16_cast_fp16)[name = tensor<string, []>("input_49_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_2_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(33305216)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34484928)))];
            tensor<fp16, [1, 512, 384]> linear_17_cast_fp16 = linear(bias = model_encoder_layer_2_output_dense_bias_to_fp16, weight = model_encoder_layer_2_output_dense_weight_to_fp16, x = input_49_cast_fp16)[name = tensor<string, []>("linear_17_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_53_cast_fp16 = add(x = linear_17_cast_fp16, y = input_45_cast_fp16)[name = tensor<string, []>("input_53_cast_fp16")];
            tensor<int32, [1]> hidden_states_19_axes_0 = const()[name = tensor<string, []>("hidden_states_19_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34485760)))];
            tensor<fp16, [384]> model_encoder_layer_2_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_2_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34486592)))];
            tensor<fp16, [1, 512, 384]> hidden_states_19_cast_fp16 = layer_norm(axes = hidden_states_19_axes_0, beta = model_encoder_layer_2_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_2_output_LayerNorm_weight_to_fp16, x = input_53_cast_fp16)[name = tensor<string, []>("hidden_states_19_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34487424)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34782400)))];
            tensor<fp16, [1, 512, 384]> linear_18_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_query_bias_to_fp16, weight = model_encoder_layer_3_attention_self_query_weight_to_fp16, x = hidden_states_19_cast_fp16)[name = tensor<string, []>("linear_18_cast_fp16")];
            tensor<int32, [4]> var_308 = const()[name = tensor<string, []>("op_308"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_39_cast_fp16 = reshape(shape = var_308, x = linear_18_cast_fp16)[name = tensor<string, []>("x_39_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(34783232)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35078208)))];
            tensor<fp16, [1, 512, 384]> linear_19_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_key_bias_to_fp16, weight = model_encoder_layer_3_attention_self_key_weight_to_fp16, x = hidden_states_19_cast_fp16)[name = tensor<string, []>("linear_19_cast_fp16")];
            tensor<int32, [4]> var_317 = const()[name = tensor<string, []>("op_317"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_43_cast_fp16 = reshape(shape = var_317, x = linear_19_cast_fp16)[name = tensor<string, []>("x_43_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35079040)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35374016)))];
            tensor<fp16, [1, 512, 384]> linear_20_cast_fp16 = linear(bias = model_encoder_layer_3_attention_self_value_bias_to_fp16, weight = model_encoder_layer_3_attention_self_value_weight_to_fp16, x = hidden_states_19_cast_fp16)[name = tensor<string, []>("linear_20_cast_fp16")];
            tensor<int32, [4]> var_326 = const()[name = tensor<string, []>("op_326"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_47_cast_fp16 = reshape(shape = var_326, x = linear_20_cast_fp16)[name = tensor<string, []>("x_47_cast_fp16")];
            tensor<int32, [4]> var_328 = const()[name = tensor<string, []>("op_328"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_3_y_0_to_fp16 = const()[name = tensor<string, []>("mul_3_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_3_cast_fp16 = mul(x = x_39_cast_fp16, y = mul_3_y_0_to_fp16)[name = tensor<string, []>("mul_3_cast_fp16")];
            tensor<bool, []> matmul_3_transpose_y_0 = const()[name = tensor<string, []>("matmul_3_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_3_transpose_x_0 = const()[name = tensor<string, []>("matmul_3_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_54_perm_0 = const()[name = tensor<string, []>("transpose_54_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_55_perm_0 = const()[name = tensor<string, []>("transpose_55_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_55 = transpose(perm = transpose_55_perm_0, x = x_43_cast_fp16)[name = tensor<string, []>("transpose_105")];
            tensor<fp16, [1, 12, 512, 32]> transpose_54 = transpose(perm = transpose_54_perm_0, x = mul_3_cast_fp16)[name = tensor<string, []>("transpose_106")];
            tensor<fp16, [1, 12, 512, 512]> matmul_3_cast_fp16 = matmul(transpose_x = matmul_3_transpose_x_0, transpose_y = matmul_3_transpose_y_0, x = transpose_54, y = transpose_55)[name = tensor<string, []>("matmul_3_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_3_cast_fp16 = add(x = matmul_3_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_3_cast_fp16")];
            tensor<int32, []> softmax_3_axis_0 = const()[name = tensor<string, []>("softmax_3_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_3_cast_fp16 = softmax(axis = softmax_3_axis_0, x = add_3_cast_fp16)[name = tensor<string, []>("softmax_3_cast_fp16")];
            tensor<bool, []> attn_output_13_transpose_x_0 = const()[name = tensor<string, []>("attn_output_13_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_13_transpose_y_0 = const()[name = tensor<string, []>("attn_output_13_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_7_cast_fp16 = transpose(perm = var_328, x = x_47_cast_fp16)[name = tensor<string, []>("transpose_107")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_13_cast_fp16 = matmul(transpose_x = attn_output_13_transpose_x_0, transpose_y = attn_output_13_transpose_y_0, x = softmax_3_cast_fp16, y = value_layer_7_cast_fp16)[name = tensor<string, []>("attn_output_13_cast_fp16")];
            tensor<int32, [4]> attn_output_15_perm_0 = const()[name = tensor<string, []>("attn_output_15_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_332 = const()[name = tensor<string, []>("op_332"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_15_cast_fp16 = transpose(perm = attn_output_15_perm_0, x = attn_output_13_cast_fp16)[name = tensor<string, []>("transpose_104")];
            tensor<fp16, [1, 512, 384]> input_55_cast_fp16 = reshape(shape = var_332, x = attn_output_15_cast_fp16)[name = tensor<string, []>("input_55_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_3_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35374848)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35669824)))];
            tensor<fp16, [1, 512, 384]> linear_21_cast_fp16 = linear(bias = model_encoder_layer_3_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_3_attention_output_dense_weight_to_fp16, x = input_55_cast_fp16)[name = tensor<string, []>("linear_21_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_59_cast_fp16 = add(x = linear_21_cast_fp16, y = hidden_states_19_cast_fp16)[name = tensor<string, []>("input_59_cast_fp16")];
            tensor<int32, [1]> input_61_axes_0 = const()[name = tensor<string, []>("input_61_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35670656)))];
            tensor<fp16, [384]> model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35671488)))];
            tensor<fp16, [1, 512, 384]> input_61_cast_fp16 = layer_norm(axes = input_61_axes_0, beta = model_encoder_layer_3_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_3_attention_output_LayerNorm_weight_to_fp16, x = input_59_cast_fp16)[name = tensor<string, []>("input_61_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_3_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(35672320)))];
            tensor<fp16, [1536]> model_encoder_layer_3_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36852032)))];
            tensor<fp16, [1, 512, 1536]> linear_22_cast_fp16 = linear(bias = model_encoder_layer_3_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_3_intermediate_dense_weight_to_fp16, x = input_61_cast_fp16)[name = tensor<string, []>("linear_22_cast_fp16")];
            tensor<string, []> input_65_mode_0 = const()[name = tensor<string, []>("input_65_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_65_cast_fp16 = gelu(mode = input_65_mode_0, x = linear_22_cast_fp16)[name = tensor<string, []>("input_65_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_3_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(36855168)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38034880)))];
            tensor<fp16, [1, 512, 384]> linear_23_cast_fp16 = linear(bias = model_encoder_layer_3_output_dense_bias_to_fp16, weight = model_encoder_layer_3_output_dense_weight_to_fp16, x = input_65_cast_fp16)[name = tensor<string, []>("linear_23_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_69_cast_fp16 = add(x = linear_23_cast_fp16, y = input_61_cast_fp16)[name = tensor<string, []>("input_69_cast_fp16")];
            tensor<int32, [1]> hidden_states_25_axes_0 = const()[name = tensor<string, []>("hidden_states_25_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38035712)))];
            tensor<fp16, [384]> model_encoder_layer_3_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_3_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38036544)))];
            tensor<fp16, [1, 512, 384]> hidden_states_25_cast_fp16 = layer_norm(axes = hidden_states_25_axes_0, beta = model_encoder_layer_3_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_3_output_LayerNorm_weight_to_fp16, x = input_69_cast_fp16)[name = tensor<string, []>("hidden_states_25_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38037376)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38332352)))];
            tensor<fp16, [1, 512, 384]> linear_24_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_query_bias_to_fp16, weight = model_encoder_layer_4_attention_self_query_weight_to_fp16, x = hidden_states_25_cast_fp16)[name = tensor<string, []>("linear_24_cast_fp16")];
            tensor<int32, [4]> var_376 = const()[name = tensor<string, []>("op_376"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_51_cast_fp16 = reshape(shape = var_376, x = linear_24_cast_fp16)[name = tensor<string, []>("x_51_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38333184)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38628160)))];
            tensor<fp16, [1, 512, 384]> linear_25_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_key_bias_to_fp16, weight = model_encoder_layer_4_attention_self_key_weight_to_fp16, x = hidden_states_25_cast_fp16)[name = tensor<string, []>("linear_25_cast_fp16")];
            tensor<int32, [4]> var_385 = const()[name = tensor<string, []>("op_385"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_55_cast_fp16 = reshape(shape = var_385, x = linear_25_cast_fp16)[name = tensor<string, []>("x_55_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38628992)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38923968)))];
            tensor<fp16, [1, 512, 384]> linear_26_cast_fp16 = linear(bias = model_encoder_layer_4_attention_self_value_bias_to_fp16, weight = model_encoder_layer_4_attention_self_value_weight_to_fp16, x = hidden_states_25_cast_fp16)[name = tensor<string, []>("linear_26_cast_fp16")];
            tensor<int32, [4]> var_394 = const()[name = tensor<string, []>("op_394"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_59_cast_fp16 = reshape(shape = var_394, x = linear_26_cast_fp16)[name = tensor<string, []>("x_59_cast_fp16")];
            tensor<int32, [4]> var_396 = const()[name = tensor<string, []>("op_396"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_4_y_0_to_fp16 = const()[name = tensor<string, []>("mul_4_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_4_cast_fp16 = mul(x = x_51_cast_fp16, y = mul_4_y_0_to_fp16)[name = tensor<string, []>("mul_4_cast_fp16")];
            tensor<bool, []> matmul_4_transpose_y_0 = const()[name = tensor<string, []>("matmul_4_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_4_transpose_x_0 = const()[name = tensor<string, []>("matmul_4_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_56_perm_0 = const()[name = tensor<string, []>("transpose_56_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_57_perm_0 = const()[name = tensor<string, []>("transpose_57_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_57 = transpose(perm = transpose_57_perm_0, x = x_55_cast_fp16)[name = tensor<string, []>("transpose_101")];
            tensor<fp16, [1, 12, 512, 32]> transpose_56 = transpose(perm = transpose_56_perm_0, x = mul_4_cast_fp16)[name = tensor<string, []>("transpose_102")];
            tensor<fp16, [1, 12, 512, 512]> matmul_4_cast_fp16 = matmul(transpose_x = matmul_4_transpose_x_0, transpose_y = matmul_4_transpose_y_0, x = transpose_56, y = transpose_57)[name = tensor<string, []>("matmul_4_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_4_cast_fp16 = add(x = matmul_4_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_4_cast_fp16")];
            tensor<int32, []> softmax_4_axis_0 = const()[name = tensor<string, []>("softmax_4_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_4_cast_fp16 = softmax(axis = softmax_4_axis_0, x = add_4_cast_fp16)[name = tensor<string, []>("softmax_4_cast_fp16")];
            tensor<bool, []> attn_output_17_transpose_x_0 = const()[name = tensor<string, []>("attn_output_17_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_17_transpose_y_0 = const()[name = tensor<string, []>("attn_output_17_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_9_cast_fp16 = transpose(perm = var_396, x = x_59_cast_fp16)[name = tensor<string, []>("transpose_103")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_17_cast_fp16 = matmul(transpose_x = attn_output_17_transpose_x_0, transpose_y = attn_output_17_transpose_y_0, x = softmax_4_cast_fp16, y = value_layer_9_cast_fp16)[name = tensor<string, []>("attn_output_17_cast_fp16")];
            tensor<int32, [4]> attn_output_19_perm_0 = const()[name = tensor<string, []>("attn_output_19_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_400 = const()[name = tensor<string, []>("op_400"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_19_cast_fp16 = transpose(perm = attn_output_19_perm_0, x = attn_output_17_cast_fp16)[name = tensor<string, []>("transpose_100")];
            tensor<fp16, [1, 512, 384]> input_71_cast_fp16 = reshape(shape = var_400, x = attn_output_19_cast_fp16)[name = tensor<string, []>("input_71_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_4_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(38924800)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39219776)))];
            tensor<fp16, [1, 512, 384]> linear_27_cast_fp16 = linear(bias = model_encoder_layer_4_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_4_attention_output_dense_weight_to_fp16, x = input_71_cast_fp16)[name = tensor<string, []>("linear_27_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_75_cast_fp16 = add(x = linear_27_cast_fp16, y = hidden_states_25_cast_fp16)[name = tensor<string, []>("input_75_cast_fp16")];
            tensor<int32, [1]> input_77_axes_0 = const()[name = tensor<string, []>("input_77_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39220608)))];
            tensor<fp16, [384]> model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39221440)))];
            tensor<fp16, [1, 512, 384]> input_77_cast_fp16 = layer_norm(axes = input_77_axes_0, beta = model_encoder_layer_4_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_4_attention_output_LayerNorm_weight_to_fp16, x = input_75_cast_fp16)[name = tensor<string, []>("input_77_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_4_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(39222272)))];
            tensor<fp16, [1536]> model_encoder_layer_4_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40401984)))];
            tensor<fp16, [1, 512, 1536]> linear_28_cast_fp16 = linear(bias = model_encoder_layer_4_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_4_intermediate_dense_weight_to_fp16, x = input_77_cast_fp16)[name = tensor<string, []>("linear_28_cast_fp16")];
            tensor<string, []> input_81_mode_0 = const()[name = tensor<string, []>("input_81_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_81_cast_fp16 = gelu(mode = input_81_mode_0, x = linear_28_cast_fp16)[name = tensor<string, []>("input_81_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_4_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(40405120)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41584832)))];
            tensor<fp16, [1, 512, 384]> linear_29_cast_fp16 = linear(bias = model_encoder_layer_4_output_dense_bias_to_fp16, weight = model_encoder_layer_4_output_dense_weight_to_fp16, x = input_81_cast_fp16)[name = tensor<string, []>("linear_29_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_85_cast_fp16 = add(x = linear_29_cast_fp16, y = input_77_cast_fp16)[name = tensor<string, []>("input_85_cast_fp16")];
            tensor<int32, [1]> hidden_states_31_axes_0 = const()[name = tensor<string, []>("hidden_states_31_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41585664)))];
            tensor<fp16, [384]> model_encoder_layer_4_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_4_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41586496)))];
            tensor<fp16, [1, 512, 384]> hidden_states_31_cast_fp16 = layer_norm(axes = hidden_states_31_axes_0, beta = model_encoder_layer_4_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_4_output_LayerNorm_weight_to_fp16, x = input_85_cast_fp16)[name = tensor<string, []>("hidden_states_31_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41587328)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41882304)))];
            tensor<fp16, [1, 512, 384]> linear_30_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_query_bias_to_fp16, weight = model_encoder_layer_5_attention_self_query_weight_to_fp16, x = hidden_states_31_cast_fp16)[name = tensor<string, []>("linear_30_cast_fp16")];
            tensor<int32, [4]> var_444 = const()[name = tensor<string, []>("op_444"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_63_cast_fp16 = reshape(shape = var_444, x = linear_30_cast_fp16)[name = tensor<string, []>("x_63_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(41883136)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42178112)))];
            tensor<fp16, [1, 512, 384]> linear_31_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_key_bias_to_fp16, weight = model_encoder_layer_5_attention_self_key_weight_to_fp16, x = hidden_states_31_cast_fp16)[name = tensor<string, []>("linear_31_cast_fp16")];
            tensor<int32, [4]> var_453 = const()[name = tensor<string, []>("op_453"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_67_cast_fp16 = reshape(shape = var_453, x = linear_31_cast_fp16)[name = tensor<string, []>("x_67_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42178944)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42473920)))];
            tensor<fp16, [1, 512, 384]> linear_32_cast_fp16 = linear(bias = model_encoder_layer_5_attention_self_value_bias_to_fp16, weight = model_encoder_layer_5_attention_self_value_weight_to_fp16, x = hidden_states_31_cast_fp16)[name = tensor<string, []>("linear_32_cast_fp16")];
            tensor<int32, [4]> var_462 = const()[name = tensor<string, []>("op_462"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_71_cast_fp16 = reshape(shape = var_462, x = linear_32_cast_fp16)[name = tensor<string, []>("x_71_cast_fp16")];
            tensor<int32, [4]> var_464 = const()[name = tensor<string, []>("op_464"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_5_y_0_to_fp16 = const()[name = tensor<string, []>("mul_5_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_5_cast_fp16 = mul(x = x_63_cast_fp16, y = mul_5_y_0_to_fp16)[name = tensor<string, []>("mul_5_cast_fp16")];
            tensor<bool, []> matmul_5_transpose_y_0 = const()[name = tensor<string, []>("matmul_5_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_5_transpose_x_0 = const()[name = tensor<string, []>("matmul_5_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_58_perm_0 = const()[name = tensor<string, []>("transpose_58_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_59_perm_0 = const()[name = tensor<string, []>("transpose_59_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_59 = transpose(perm = transpose_59_perm_0, x = x_67_cast_fp16)[name = tensor<string, []>("transpose_97")];
            tensor<fp16, [1, 12, 512, 32]> transpose_58 = transpose(perm = transpose_58_perm_0, x = mul_5_cast_fp16)[name = tensor<string, []>("transpose_98")];
            tensor<fp16, [1, 12, 512, 512]> matmul_5_cast_fp16 = matmul(transpose_x = matmul_5_transpose_x_0, transpose_y = matmul_5_transpose_y_0, x = transpose_58, y = transpose_59)[name = tensor<string, []>("matmul_5_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_5_cast_fp16 = add(x = matmul_5_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_5_cast_fp16")];
            tensor<int32, []> softmax_5_axis_0 = const()[name = tensor<string, []>("softmax_5_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_5_cast_fp16 = softmax(axis = softmax_5_axis_0, x = add_5_cast_fp16)[name = tensor<string, []>("softmax_5_cast_fp16")];
            tensor<bool, []> attn_output_21_transpose_x_0 = const()[name = tensor<string, []>("attn_output_21_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_21_transpose_y_0 = const()[name = tensor<string, []>("attn_output_21_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_11_cast_fp16 = transpose(perm = var_464, x = x_71_cast_fp16)[name = tensor<string, []>("transpose_99")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_21_cast_fp16 = matmul(transpose_x = attn_output_21_transpose_x_0, transpose_y = attn_output_21_transpose_y_0, x = softmax_5_cast_fp16, y = value_layer_11_cast_fp16)[name = tensor<string, []>("attn_output_21_cast_fp16")];
            tensor<int32, [4]> attn_output_23_perm_0 = const()[name = tensor<string, []>("attn_output_23_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_468 = const()[name = tensor<string, []>("op_468"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_23_cast_fp16 = transpose(perm = attn_output_23_perm_0, x = attn_output_21_cast_fp16)[name = tensor<string, []>("transpose_96")];
            tensor<fp16, [1, 512, 384]> input_87_cast_fp16 = reshape(shape = var_468, x = attn_output_23_cast_fp16)[name = tensor<string, []>("input_87_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_5_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42474752)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42769728)))];
            tensor<fp16, [1, 512, 384]> linear_33_cast_fp16 = linear(bias = model_encoder_layer_5_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_5_attention_output_dense_weight_to_fp16, x = input_87_cast_fp16)[name = tensor<string, []>("linear_33_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_91_cast_fp16 = add(x = linear_33_cast_fp16, y = hidden_states_31_cast_fp16)[name = tensor<string, []>("input_91_cast_fp16")];
            tensor<int32, [1]> input_93_axes_0 = const()[name = tensor<string, []>("input_93_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42770560)))];
            tensor<fp16, [384]> model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42771392)))];
            tensor<fp16, [1, 512, 384]> input_93_cast_fp16 = layer_norm(axes = input_93_axes_0, beta = model_encoder_layer_5_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_5_attention_output_LayerNorm_weight_to_fp16, x = input_91_cast_fp16)[name = tensor<string, []>("input_93_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_5_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(42772224)))];
            tensor<fp16, [1536]> model_encoder_layer_5_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43951936)))];
            tensor<fp16, [1, 512, 1536]> linear_34_cast_fp16 = linear(bias = model_encoder_layer_5_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_5_intermediate_dense_weight_to_fp16, x = input_93_cast_fp16)[name = tensor<string, []>("linear_34_cast_fp16")];
            tensor<string, []> input_97_mode_0 = const()[name = tensor<string, []>("input_97_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_97_cast_fp16 = gelu(mode = input_97_mode_0, x = linear_34_cast_fp16)[name = tensor<string, []>("input_97_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_5_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(43955072)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45134784)))];
            tensor<fp16, [1, 512, 384]> linear_35_cast_fp16 = linear(bias = model_encoder_layer_5_output_dense_bias_to_fp16, weight = model_encoder_layer_5_output_dense_weight_to_fp16, x = input_97_cast_fp16)[name = tensor<string, []>("linear_35_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_101_cast_fp16 = add(x = linear_35_cast_fp16, y = input_93_cast_fp16)[name = tensor<string, []>("input_101_cast_fp16")];
            tensor<int32, [1]> hidden_states_37_axes_0 = const()[name = tensor<string, []>("hidden_states_37_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45135616)))];
            tensor<fp16, [384]> model_encoder_layer_5_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_5_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45136448)))];
            tensor<fp16, [1, 512, 384]> hidden_states_37_cast_fp16 = layer_norm(axes = hidden_states_37_axes_0, beta = model_encoder_layer_5_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_5_output_LayerNorm_weight_to_fp16, x = input_101_cast_fp16)[name = tensor<string, []>("hidden_states_37_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_6_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45137280)))];
            tensor<fp16, [384]> model_encoder_layer_6_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45432256)))];
            tensor<fp16, [1, 512, 384]> linear_36_cast_fp16 = linear(bias = model_encoder_layer_6_attention_self_query_bias_to_fp16, weight = model_encoder_layer_6_attention_self_query_weight_to_fp16, x = hidden_states_37_cast_fp16)[name = tensor<string, []>("linear_36_cast_fp16")];
            tensor<int32, [4]> var_512 = const()[name = tensor<string, []>("op_512"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_75_cast_fp16 = reshape(shape = var_512, x = linear_36_cast_fp16)[name = tensor<string, []>("x_75_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_6_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45433088)))];
            tensor<fp16, [384]> model_encoder_layer_6_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45728064)))];
            tensor<fp16, [1, 512, 384]> linear_37_cast_fp16 = linear(bias = model_encoder_layer_6_attention_self_key_bias_to_fp16, weight = model_encoder_layer_6_attention_self_key_weight_to_fp16, x = hidden_states_37_cast_fp16)[name = tensor<string, []>("linear_37_cast_fp16")];
            tensor<int32, [4]> var_521 = const()[name = tensor<string, []>("op_521"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_79_cast_fp16 = reshape(shape = var_521, x = linear_37_cast_fp16)[name = tensor<string, []>("x_79_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_6_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(45728896)))];
            tensor<fp16, [384]> model_encoder_layer_6_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46023872)))];
            tensor<fp16, [1, 512, 384]> linear_38_cast_fp16 = linear(bias = model_encoder_layer_6_attention_self_value_bias_to_fp16, weight = model_encoder_layer_6_attention_self_value_weight_to_fp16, x = hidden_states_37_cast_fp16)[name = tensor<string, []>("linear_38_cast_fp16")];
            tensor<int32, [4]> var_530 = const()[name = tensor<string, []>("op_530"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_83_cast_fp16 = reshape(shape = var_530, x = linear_38_cast_fp16)[name = tensor<string, []>("x_83_cast_fp16")];
            tensor<int32, [4]> var_532 = const()[name = tensor<string, []>("op_532"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_6_y_0_to_fp16 = const()[name = tensor<string, []>("mul_6_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_6_cast_fp16 = mul(x = x_75_cast_fp16, y = mul_6_y_0_to_fp16)[name = tensor<string, []>("mul_6_cast_fp16")];
            tensor<bool, []> matmul_6_transpose_y_0 = const()[name = tensor<string, []>("matmul_6_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_6_transpose_x_0 = const()[name = tensor<string, []>("matmul_6_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_60_perm_0 = const()[name = tensor<string, []>("transpose_60_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_61_perm_0 = const()[name = tensor<string, []>("transpose_61_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_61 = transpose(perm = transpose_61_perm_0, x = x_79_cast_fp16)[name = tensor<string, []>("transpose_93")];
            tensor<fp16, [1, 12, 512, 32]> transpose_60 = transpose(perm = transpose_60_perm_0, x = mul_6_cast_fp16)[name = tensor<string, []>("transpose_94")];
            tensor<fp16, [1, 12, 512, 512]> matmul_6_cast_fp16 = matmul(transpose_x = matmul_6_transpose_x_0, transpose_y = matmul_6_transpose_y_0, x = transpose_60, y = transpose_61)[name = tensor<string, []>("matmul_6_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_6_cast_fp16 = add(x = matmul_6_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_6_cast_fp16")];
            tensor<int32, []> softmax_6_axis_0 = const()[name = tensor<string, []>("softmax_6_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_6_cast_fp16 = softmax(axis = softmax_6_axis_0, x = add_6_cast_fp16)[name = tensor<string, []>("softmax_6_cast_fp16")];
            tensor<bool, []> attn_output_25_transpose_x_0 = const()[name = tensor<string, []>("attn_output_25_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_25_transpose_y_0 = const()[name = tensor<string, []>("attn_output_25_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_13_cast_fp16 = transpose(perm = var_532, x = x_83_cast_fp16)[name = tensor<string, []>("transpose_95")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_25_cast_fp16 = matmul(transpose_x = attn_output_25_transpose_x_0, transpose_y = attn_output_25_transpose_y_0, x = softmax_6_cast_fp16, y = value_layer_13_cast_fp16)[name = tensor<string, []>("attn_output_25_cast_fp16")];
            tensor<int32, [4]> attn_output_27_perm_0 = const()[name = tensor<string, []>("attn_output_27_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_536 = const()[name = tensor<string, []>("op_536"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_27_cast_fp16 = transpose(perm = attn_output_27_perm_0, x = attn_output_25_cast_fp16)[name = tensor<string, []>("transpose_92")];
            tensor<fp16, [1, 512, 384]> input_103_cast_fp16 = reshape(shape = var_536, x = attn_output_27_cast_fp16)[name = tensor<string, []>("input_103_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_6_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46024704)))];
            tensor<fp16, [384]> model_encoder_layer_6_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46319680)))];
            tensor<fp16, [1, 512, 384]> linear_39_cast_fp16 = linear(bias = model_encoder_layer_6_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_6_attention_output_dense_weight_to_fp16, x = input_103_cast_fp16)[name = tensor<string, []>("linear_39_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_107_cast_fp16 = add(x = linear_39_cast_fp16, y = hidden_states_37_cast_fp16)[name = tensor<string, []>("input_107_cast_fp16")];
            tensor<int32, [1]> input_109_axes_0 = const()[name = tensor<string, []>("input_109_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_6_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46320512)))];
            tensor<fp16, [384]> model_encoder_layer_6_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46321344)))];
            tensor<fp16, [1, 512, 384]> input_109_cast_fp16 = layer_norm(axes = input_109_axes_0, beta = model_encoder_layer_6_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_6_attention_output_LayerNorm_weight_to_fp16, x = input_107_cast_fp16)[name = tensor<string, []>("input_109_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_6_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(46322176)))];
            tensor<fp16, [1536]> model_encoder_layer_6_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47501888)))];
            tensor<fp16, [1, 512, 1536]> linear_40_cast_fp16 = linear(bias = model_encoder_layer_6_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_6_intermediate_dense_weight_to_fp16, x = input_109_cast_fp16)[name = tensor<string, []>("linear_40_cast_fp16")];
            tensor<string, []> input_113_mode_0 = const()[name = tensor<string, []>("input_113_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_113_cast_fp16 = gelu(mode = input_113_mode_0, x = linear_40_cast_fp16)[name = tensor<string, []>("input_113_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_6_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(47505024)))];
            tensor<fp16, [384]> model_encoder_layer_6_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48684736)))];
            tensor<fp16, [1, 512, 384]> linear_41_cast_fp16 = linear(bias = model_encoder_layer_6_output_dense_bias_to_fp16, weight = model_encoder_layer_6_output_dense_weight_to_fp16, x = input_113_cast_fp16)[name = tensor<string, []>("linear_41_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_117_cast_fp16 = add(x = linear_41_cast_fp16, y = input_109_cast_fp16)[name = tensor<string, []>("input_117_cast_fp16")];
            tensor<int32, [1]> hidden_states_43_axes_0 = const()[name = tensor<string, []>("hidden_states_43_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_6_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48685568)))];
            tensor<fp16, [384]> model_encoder_layer_6_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_6_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48686400)))];
            tensor<fp16, [1, 512, 384]> hidden_states_43_cast_fp16 = layer_norm(axes = hidden_states_43_axes_0, beta = model_encoder_layer_6_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_6_output_LayerNorm_weight_to_fp16, x = input_117_cast_fp16)[name = tensor<string, []>("hidden_states_43_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_7_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48687232)))];
            tensor<fp16, [384]> model_encoder_layer_7_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48982208)))];
            tensor<fp16, [1, 512, 384]> linear_42_cast_fp16 = linear(bias = model_encoder_layer_7_attention_self_query_bias_to_fp16, weight = model_encoder_layer_7_attention_self_query_weight_to_fp16, x = hidden_states_43_cast_fp16)[name = tensor<string, []>("linear_42_cast_fp16")];
            tensor<int32, [4]> var_580 = const()[name = tensor<string, []>("op_580"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_87_cast_fp16 = reshape(shape = var_580, x = linear_42_cast_fp16)[name = tensor<string, []>("x_87_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_7_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(48983040)))];
            tensor<fp16, [384]> model_encoder_layer_7_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49278016)))];
            tensor<fp16, [1, 512, 384]> linear_43_cast_fp16 = linear(bias = model_encoder_layer_7_attention_self_key_bias_to_fp16, weight = model_encoder_layer_7_attention_self_key_weight_to_fp16, x = hidden_states_43_cast_fp16)[name = tensor<string, []>("linear_43_cast_fp16")];
            tensor<int32, [4]> var_589 = const()[name = tensor<string, []>("op_589"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_91_cast_fp16 = reshape(shape = var_589, x = linear_43_cast_fp16)[name = tensor<string, []>("x_91_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_7_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49278848)))];
            tensor<fp16, [384]> model_encoder_layer_7_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49573824)))];
            tensor<fp16, [1, 512, 384]> linear_44_cast_fp16 = linear(bias = model_encoder_layer_7_attention_self_value_bias_to_fp16, weight = model_encoder_layer_7_attention_self_value_weight_to_fp16, x = hidden_states_43_cast_fp16)[name = tensor<string, []>("linear_44_cast_fp16")];
            tensor<int32, [4]> var_598 = const()[name = tensor<string, []>("op_598"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_95_cast_fp16 = reshape(shape = var_598, x = linear_44_cast_fp16)[name = tensor<string, []>("x_95_cast_fp16")];
            tensor<int32, [4]> var_600 = const()[name = tensor<string, []>("op_600"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_7_y_0_to_fp16 = const()[name = tensor<string, []>("mul_7_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_7_cast_fp16 = mul(x = x_87_cast_fp16, y = mul_7_y_0_to_fp16)[name = tensor<string, []>("mul_7_cast_fp16")];
            tensor<bool, []> matmul_7_transpose_y_0 = const()[name = tensor<string, []>("matmul_7_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_7_transpose_x_0 = const()[name = tensor<string, []>("matmul_7_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_62_perm_0 = const()[name = tensor<string, []>("transpose_62_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_63_perm_0 = const()[name = tensor<string, []>("transpose_63_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_63 = transpose(perm = transpose_63_perm_0, x = x_91_cast_fp16)[name = tensor<string, []>("transpose_89")];
            tensor<fp16, [1, 12, 512, 32]> transpose_62 = transpose(perm = transpose_62_perm_0, x = mul_7_cast_fp16)[name = tensor<string, []>("transpose_90")];
            tensor<fp16, [1, 12, 512, 512]> matmul_7_cast_fp16 = matmul(transpose_x = matmul_7_transpose_x_0, transpose_y = matmul_7_transpose_y_0, x = transpose_62, y = transpose_63)[name = tensor<string, []>("matmul_7_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_7_cast_fp16 = add(x = matmul_7_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_7_cast_fp16")];
            tensor<int32, []> softmax_7_axis_0 = const()[name = tensor<string, []>("softmax_7_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_7_cast_fp16 = softmax(axis = softmax_7_axis_0, x = add_7_cast_fp16)[name = tensor<string, []>("softmax_7_cast_fp16")];
            tensor<bool, []> attn_output_29_transpose_x_0 = const()[name = tensor<string, []>("attn_output_29_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_29_transpose_y_0 = const()[name = tensor<string, []>("attn_output_29_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_15_cast_fp16 = transpose(perm = var_600, x = x_95_cast_fp16)[name = tensor<string, []>("transpose_91")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_29_cast_fp16 = matmul(transpose_x = attn_output_29_transpose_x_0, transpose_y = attn_output_29_transpose_y_0, x = softmax_7_cast_fp16, y = value_layer_15_cast_fp16)[name = tensor<string, []>("attn_output_29_cast_fp16")];
            tensor<int32, [4]> attn_output_31_perm_0 = const()[name = tensor<string, []>("attn_output_31_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_604 = const()[name = tensor<string, []>("op_604"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_31_cast_fp16 = transpose(perm = attn_output_31_perm_0, x = attn_output_29_cast_fp16)[name = tensor<string, []>("transpose_88")];
            tensor<fp16, [1, 512, 384]> input_119_cast_fp16 = reshape(shape = var_604, x = attn_output_31_cast_fp16)[name = tensor<string, []>("input_119_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_7_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49574656)))];
            tensor<fp16, [384]> model_encoder_layer_7_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49869632)))];
            tensor<fp16, [1, 512, 384]> linear_45_cast_fp16 = linear(bias = model_encoder_layer_7_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_7_attention_output_dense_weight_to_fp16, x = input_119_cast_fp16)[name = tensor<string, []>("linear_45_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_123_cast_fp16 = add(x = linear_45_cast_fp16, y = hidden_states_43_cast_fp16)[name = tensor<string, []>("input_123_cast_fp16")];
            tensor<int32, [1]> input_125_axes_0 = const()[name = tensor<string, []>("input_125_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_7_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49870464)))];
            tensor<fp16, [384]> model_encoder_layer_7_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49871296)))];
            tensor<fp16, [1, 512, 384]> input_125_cast_fp16 = layer_norm(axes = input_125_axes_0, beta = model_encoder_layer_7_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_7_attention_output_LayerNorm_weight_to_fp16, x = input_123_cast_fp16)[name = tensor<string, []>("input_125_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_7_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(49872128)))];
            tensor<fp16, [1536]> model_encoder_layer_7_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51051840)))];
            tensor<fp16, [1, 512, 1536]> linear_46_cast_fp16 = linear(bias = model_encoder_layer_7_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_7_intermediate_dense_weight_to_fp16, x = input_125_cast_fp16)[name = tensor<string, []>("linear_46_cast_fp16")];
            tensor<string, []> input_129_mode_0 = const()[name = tensor<string, []>("input_129_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_129_cast_fp16 = gelu(mode = input_129_mode_0, x = linear_46_cast_fp16)[name = tensor<string, []>("input_129_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_7_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(51054976)))];
            tensor<fp16, [384]> model_encoder_layer_7_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52234688)))];
            tensor<fp16, [1, 512, 384]> linear_47_cast_fp16 = linear(bias = model_encoder_layer_7_output_dense_bias_to_fp16, weight = model_encoder_layer_7_output_dense_weight_to_fp16, x = input_129_cast_fp16)[name = tensor<string, []>("linear_47_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_133_cast_fp16 = add(x = linear_47_cast_fp16, y = input_125_cast_fp16)[name = tensor<string, []>("input_133_cast_fp16")];
            tensor<int32, [1]> hidden_states_49_axes_0 = const()[name = tensor<string, []>("hidden_states_49_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_7_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52235520)))];
            tensor<fp16, [384]> model_encoder_layer_7_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_7_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52236352)))];
            tensor<fp16, [1, 512, 384]> hidden_states_49_cast_fp16 = layer_norm(axes = hidden_states_49_axes_0, beta = model_encoder_layer_7_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_7_output_LayerNorm_weight_to_fp16, x = input_133_cast_fp16)[name = tensor<string, []>("hidden_states_49_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_8_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52237184)))];
            tensor<fp16, [384]> model_encoder_layer_8_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52532160)))];
            tensor<fp16, [1, 512, 384]> linear_48_cast_fp16 = linear(bias = model_encoder_layer_8_attention_self_query_bias_to_fp16, weight = model_encoder_layer_8_attention_self_query_weight_to_fp16, x = hidden_states_49_cast_fp16)[name = tensor<string, []>("linear_48_cast_fp16")];
            tensor<int32, [4]> var_648 = const()[name = tensor<string, []>("op_648"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_99_cast_fp16 = reshape(shape = var_648, x = linear_48_cast_fp16)[name = tensor<string, []>("x_99_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_8_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52532992)))];
            tensor<fp16, [384]> model_encoder_layer_8_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52827968)))];
            tensor<fp16, [1, 512, 384]> linear_49_cast_fp16 = linear(bias = model_encoder_layer_8_attention_self_key_bias_to_fp16, weight = model_encoder_layer_8_attention_self_key_weight_to_fp16, x = hidden_states_49_cast_fp16)[name = tensor<string, []>("linear_49_cast_fp16")];
            tensor<int32, [4]> var_657 = const()[name = tensor<string, []>("op_657"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_103_cast_fp16 = reshape(shape = var_657, x = linear_49_cast_fp16)[name = tensor<string, []>("x_103_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_8_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(52828800)))];
            tensor<fp16, [384]> model_encoder_layer_8_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(53123776)))];
            tensor<fp16, [1, 512, 384]> linear_50_cast_fp16 = linear(bias = model_encoder_layer_8_attention_self_value_bias_to_fp16, weight = model_encoder_layer_8_attention_self_value_weight_to_fp16, x = hidden_states_49_cast_fp16)[name = tensor<string, []>("linear_50_cast_fp16")];
            tensor<int32, [4]> var_666 = const()[name = tensor<string, []>("op_666"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_107_cast_fp16 = reshape(shape = var_666, x = linear_50_cast_fp16)[name = tensor<string, []>("x_107_cast_fp16")];
            tensor<int32, [4]> var_668 = const()[name = tensor<string, []>("op_668"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_8_y_0_to_fp16 = const()[name = tensor<string, []>("mul_8_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_8_cast_fp16 = mul(x = x_99_cast_fp16, y = mul_8_y_0_to_fp16)[name = tensor<string, []>("mul_8_cast_fp16")];
            tensor<bool, []> matmul_8_transpose_y_0 = const()[name = tensor<string, []>("matmul_8_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_8_transpose_x_0 = const()[name = tensor<string, []>("matmul_8_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_64_perm_0 = const()[name = tensor<string, []>("transpose_64_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_65_perm_0 = const()[name = tensor<string, []>("transpose_65_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_65 = transpose(perm = transpose_65_perm_0, x = x_103_cast_fp16)[name = tensor<string, []>("transpose_85")];
            tensor<fp16, [1, 12, 512, 32]> transpose_64 = transpose(perm = transpose_64_perm_0, x = mul_8_cast_fp16)[name = tensor<string, []>("transpose_86")];
            tensor<fp16, [1, 12, 512, 512]> matmul_8_cast_fp16 = matmul(transpose_x = matmul_8_transpose_x_0, transpose_y = matmul_8_transpose_y_0, x = transpose_64, y = transpose_65)[name = tensor<string, []>("matmul_8_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_8_cast_fp16 = add(x = matmul_8_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_8_cast_fp16")];
            tensor<int32, []> softmax_8_axis_0 = const()[name = tensor<string, []>("softmax_8_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_8_cast_fp16 = softmax(axis = softmax_8_axis_0, x = add_8_cast_fp16)[name = tensor<string, []>("softmax_8_cast_fp16")];
            tensor<bool, []> attn_output_33_transpose_x_0 = const()[name = tensor<string, []>("attn_output_33_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_33_transpose_y_0 = const()[name = tensor<string, []>("attn_output_33_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_17_cast_fp16 = transpose(perm = var_668, x = x_107_cast_fp16)[name = tensor<string, []>("transpose_87")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_33_cast_fp16 = matmul(transpose_x = attn_output_33_transpose_x_0, transpose_y = attn_output_33_transpose_y_0, x = softmax_8_cast_fp16, y = value_layer_17_cast_fp16)[name = tensor<string, []>("attn_output_33_cast_fp16")];
            tensor<int32, [4]> attn_output_35_perm_0 = const()[name = tensor<string, []>("attn_output_35_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_672 = const()[name = tensor<string, []>("op_672"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_35_cast_fp16 = transpose(perm = attn_output_35_perm_0, x = attn_output_33_cast_fp16)[name = tensor<string, []>("transpose_84")];
            tensor<fp16, [1, 512, 384]> input_135_cast_fp16 = reshape(shape = var_672, x = attn_output_35_cast_fp16)[name = tensor<string, []>("input_135_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_8_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(53124608)))];
            tensor<fp16, [384]> model_encoder_layer_8_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(53419584)))];
            tensor<fp16, [1, 512, 384]> linear_51_cast_fp16 = linear(bias = model_encoder_layer_8_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_8_attention_output_dense_weight_to_fp16, x = input_135_cast_fp16)[name = tensor<string, []>("linear_51_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_139_cast_fp16 = add(x = linear_51_cast_fp16, y = hidden_states_49_cast_fp16)[name = tensor<string, []>("input_139_cast_fp16")];
            tensor<int32, [1]> input_141_axes_0 = const()[name = tensor<string, []>("input_141_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_8_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(53420416)))];
            tensor<fp16, [384]> model_encoder_layer_8_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(53421248)))];
            tensor<fp16, [1, 512, 384]> input_141_cast_fp16 = layer_norm(axes = input_141_axes_0, beta = model_encoder_layer_8_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_8_attention_output_LayerNorm_weight_to_fp16, x = input_139_cast_fp16)[name = tensor<string, []>("input_141_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_8_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(53422080)))];
            tensor<fp16, [1536]> model_encoder_layer_8_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(54601792)))];
            tensor<fp16, [1, 512, 1536]> linear_52_cast_fp16 = linear(bias = model_encoder_layer_8_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_8_intermediate_dense_weight_to_fp16, x = input_141_cast_fp16)[name = tensor<string, []>("linear_52_cast_fp16")];
            tensor<string, []> input_145_mode_0 = const()[name = tensor<string, []>("input_145_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_145_cast_fp16 = gelu(mode = input_145_mode_0, x = linear_52_cast_fp16)[name = tensor<string, []>("input_145_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_8_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(54604928)))];
            tensor<fp16, [384]> model_encoder_layer_8_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55784640)))];
            tensor<fp16, [1, 512, 384]> linear_53_cast_fp16 = linear(bias = model_encoder_layer_8_output_dense_bias_to_fp16, weight = model_encoder_layer_8_output_dense_weight_to_fp16, x = input_145_cast_fp16)[name = tensor<string, []>("linear_53_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_149_cast_fp16 = add(x = linear_53_cast_fp16, y = input_141_cast_fp16)[name = tensor<string, []>("input_149_cast_fp16")];
            tensor<int32, [1]> hidden_states_55_axes_0 = const()[name = tensor<string, []>("hidden_states_55_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_8_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55785472)))];
            tensor<fp16, [384]> model_encoder_layer_8_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_8_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55786304)))];
            tensor<fp16, [1, 512, 384]> hidden_states_55_cast_fp16 = layer_norm(axes = hidden_states_55_axes_0, beta = model_encoder_layer_8_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_8_output_LayerNorm_weight_to_fp16, x = input_149_cast_fp16)[name = tensor<string, []>("hidden_states_55_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_9_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(55787136)))];
            tensor<fp16, [384]> model_encoder_layer_9_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56082112)))];
            tensor<fp16, [1, 512, 384]> linear_54_cast_fp16 = linear(bias = model_encoder_layer_9_attention_self_query_bias_to_fp16, weight = model_encoder_layer_9_attention_self_query_weight_to_fp16, x = hidden_states_55_cast_fp16)[name = tensor<string, []>("linear_54_cast_fp16")];
            tensor<int32, [4]> var_716 = const()[name = tensor<string, []>("op_716"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_111_cast_fp16 = reshape(shape = var_716, x = linear_54_cast_fp16)[name = tensor<string, []>("x_111_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_9_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56082944)))];
            tensor<fp16, [384]> model_encoder_layer_9_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56377920)))];
            tensor<fp16, [1, 512, 384]> linear_55_cast_fp16 = linear(bias = model_encoder_layer_9_attention_self_key_bias_to_fp16, weight = model_encoder_layer_9_attention_self_key_weight_to_fp16, x = hidden_states_55_cast_fp16)[name = tensor<string, []>("linear_55_cast_fp16")];
            tensor<int32, [4]> var_725 = const()[name = tensor<string, []>("op_725"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_115_cast_fp16 = reshape(shape = var_725, x = linear_55_cast_fp16)[name = tensor<string, []>("x_115_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_9_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56378752)))];
            tensor<fp16, [384]> model_encoder_layer_9_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56673728)))];
            tensor<fp16, [1, 512, 384]> linear_56_cast_fp16 = linear(bias = model_encoder_layer_9_attention_self_value_bias_to_fp16, weight = model_encoder_layer_9_attention_self_value_weight_to_fp16, x = hidden_states_55_cast_fp16)[name = tensor<string, []>("linear_56_cast_fp16")];
            tensor<int32, [4]> var_734 = const()[name = tensor<string, []>("op_734"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_119_cast_fp16 = reshape(shape = var_734, x = linear_56_cast_fp16)[name = tensor<string, []>("x_119_cast_fp16")];
            tensor<int32, [4]> var_736 = const()[name = tensor<string, []>("op_736"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_9_y_0_to_fp16 = const()[name = tensor<string, []>("mul_9_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_9_cast_fp16 = mul(x = x_111_cast_fp16, y = mul_9_y_0_to_fp16)[name = tensor<string, []>("mul_9_cast_fp16")];
            tensor<bool, []> matmul_9_transpose_y_0 = const()[name = tensor<string, []>("matmul_9_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_9_transpose_x_0 = const()[name = tensor<string, []>("matmul_9_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_66_perm_0 = const()[name = tensor<string, []>("transpose_66_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_67_perm_0 = const()[name = tensor<string, []>("transpose_67_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_67 = transpose(perm = transpose_67_perm_0, x = x_115_cast_fp16)[name = tensor<string, []>("transpose_81")];
            tensor<fp16, [1, 12, 512, 32]> transpose_66 = transpose(perm = transpose_66_perm_0, x = mul_9_cast_fp16)[name = tensor<string, []>("transpose_82")];
            tensor<fp16, [1, 12, 512, 512]> matmul_9_cast_fp16 = matmul(transpose_x = matmul_9_transpose_x_0, transpose_y = matmul_9_transpose_y_0, x = transpose_66, y = transpose_67)[name = tensor<string, []>("matmul_9_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_9_cast_fp16 = add(x = matmul_9_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_9_cast_fp16")];
            tensor<int32, []> softmax_9_axis_0 = const()[name = tensor<string, []>("softmax_9_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_9_cast_fp16 = softmax(axis = softmax_9_axis_0, x = add_9_cast_fp16)[name = tensor<string, []>("softmax_9_cast_fp16")];
            tensor<bool, []> attn_output_37_transpose_x_0 = const()[name = tensor<string, []>("attn_output_37_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_37_transpose_y_0 = const()[name = tensor<string, []>("attn_output_37_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_19_cast_fp16 = transpose(perm = var_736, x = x_119_cast_fp16)[name = tensor<string, []>("transpose_83")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_37_cast_fp16 = matmul(transpose_x = attn_output_37_transpose_x_0, transpose_y = attn_output_37_transpose_y_0, x = softmax_9_cast_fp16, y = value_layer_19_cast_fp16)[name = tensor<string, []>("attn_output_37_cast_fp16")];
            tensor<int32, [4]> attn_output_39_perm_0 = const()[name = tensor<string, []>("attn_output_39_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_740 = const()[name = tensor<string, []>("op_740"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_39_cast_fp16 = transpose(perm = attn_output_39_perm_0, x = attn_output_37_cast_fp16)[name = tensor<string, []>("transpose_80")];
            tensor<fp16, [1, 512, 384]> input_151_cast_fp16 = reshape(shape = var_740, x = attn_output_39_cast_fp16)[name = tensor<string, []>("input_151_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_9_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56674560)))];
            tensor<fp16, [384]> model_encoder_layer_9_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56969536)))];
            tensor<fp16, [1, 512, 384]> linear_57_cast_fp16 = linear(bias = model_encoder_layer_9_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_9_attention_output_dense_weight_to_fp16, x = input_151_cast_fp16)[name = tensor<string, []>("linear_57_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_155_cast_fp16 = add(x = linear_57_cast_fp16, y = hidden_states_55_cast_fp16)[name = tensor<string, []>("input_155_cast_fp16")];
            tensor<int32, [1]> input_157_axes_0 = const()[name = tensor<string, []>("input_157_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_9_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56970368)))];
            tensor<fp16, [384]> model_encoder_layer_9_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56971200)))];
            tensor<fp16, [1, 512, 384]> input_157_cast_fp16 = layer_norm(axes = input_157_axes_0, beta = model_encoder_layer_9_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_9_attention_output_LayerNorm_weight_to_fp16, x = input_155_cast_fp16)[name = tensor<string, []>("input_157_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_9_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(56972032)))];
            tensor<fp16, [1536]> model_encoder_layer_9_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58151744)))];
            tensor<fp16, [1, 512, 1536]> linear_58_cast_fp16 = linear(bias = model_encoder_layer_9_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_9_intermediate_dense_weight_to_fp16, x = input_157_cast_fp16)[name = tensor<string, []>("linear_58_cast_fp16")];
            tensor<string, []> input_161_mode_0 = const()[name = tensor<string, []>("input_161_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_161_cast_fp16 = gelu(mode = input_161_mode_0, x = linear_58_cast_fp16)[name = tensor<string, []>("input_161_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_9_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(58154880)))];
            tensor<fp16, [384]> model_encoder_layer_9_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59334592)))];
            tensor<fp16, [1, 512, 384]> linear_59_cast_fp16 = linear(bias = model_encoder_layer_9_output_dense_bias_to_fp16, weight = model_encoder_layer_9_output_dense_weight_to_fp16, x = input_161_cast_fp16)[name = tensor<string, []>("linear_59_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_165_cast_fp16 = add(x = linear_59_cast_fp16, y = input_157_cast_fp16)[name = tensor<string, []>("input_165_cast_fp16")];
            tensor<int32, [1]> hidden_states_61_axes_0 = const()[name = tensor<string, []>("hidden_states_61_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_9_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59335424)))];
            tensor<fp16, [384]> model_encoder_layer_9_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_9_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59336256)))];
            tensor<fp16, [1, 512, 384]> hidden_states_61_cast_fp16 = layer_norm(axes = hidden_states_61_axes_0, beta = model_encoder_layer_9_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_9_output_LayerNorm_weight_to_fp16, x = input_165_cast_fp16)[name = tensor<string, []>("hidden_states_61_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_10_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59337088)))];
            tensor<fp16, [384]> model_encoder_layer_10_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59632064)))];
            tensor<fp16, [1, 512, 384]> linear_60_cast_fp16 = linear(bias = model_encoder_layer_10_attention_self_query_bias_to_fp16, weight = model_encoder_layer_10_attention_self_query_weight_to_fp16, x = hidden_states_61_cast_fp16)[name = tensor<string, []>("linear_60_cast_fp16")];
            tensor<int32, [4]> var_784 = const()[name = tensor<string, []>("op_784"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_123_cast_fp16 = reshape(shape = var_784, x = linear_60_cast_fp16)[name = tensor<string, []>("x_123_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_10_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59632896)))];
            tensor<fp16, [384]> model_encoder_layer_10_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59927872)))];
            tensor<fp16, [1, 512, 384]> linear_61_cast_fp16 = linear(bias = model_encoder_layer_10_attention_self_key_bias_to_fp16, weight = model_encoder_layer_10_attention_self_key_weight_to_fp16, x = hidden_states_61_cast_fp16)[name = tensor<string, []>("linear_61_cast_fp16")];
            tensor<int32, [4]> var_793 = const()[name = tensor<string, []>("op_793"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_127_cast_fp16 = reshape(shape = var_793, x = linear_61_cast_fp16)[name = tensor<string, []>("x_127_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_10_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(59928704)))];
            tensor<fp16, [384]> model_encoder_layer_10_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(60223680)))];
            tensor<fp16, [1, 512, 384]> linear_62_cast_fp16 = linear(bias = model_encoder_layer_10_attention_self_value_bias_to_fp16, weight = model_encoder_layer_10_attention_self_value_weight_to_fp16, x = hidden_states_61_cast_fp16)[name = tensor<string, []>("linear_62_cast_fp16")];
            tensor<int32, [4]> var_802 = const()[name = tensor<string, []>("op_802"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_131_cast_fp16 = reshape(shape = var_802, x = linear_62_cast_fp16)[name = tensor<string, []>("x_131_cast_fp16")];
            tensor<int32, [4]> var_804 = const()[name = tensor<string, []>("op_804"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_10_y_0_to_fp16 = const()[name = tensor<string, []>("mul_10_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_10_cast_fp16 = mul(x = x_123_cast_fp16, y = mul_10_y_0_to_fp16)[name = tensor<string, []>("mul_10_cast_fp16")];
            tensor<bool, []> matmul_10_transpose_y_0 = const()[name = tensor<string, []>("matmul_10_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_10_transpose_x_0 = const()[name = tensor<string, []>("matmul_10_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_68_perm_0 = const()[name = tensor<string, []>("transpose_68_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_69_perm_0 = const()[name = tensor<string, []>("transpose_69_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_69 = transpose(perm = transpose_69_perm_0, x = x_127_cast_fp16)[name = tensor<string, []>("transpose_77")];
            tensor<fp16, [1, 12, 512, 32]> transpose_68 = transpose(perm = transpose_68_perm_0, x = mul_10_cast_fp16)[name = tensor<string, []>("transpose_78")];
            tensor<fp16, [1, 12, 512, 512]> matmul_10_cast_fp16 = matmul(transpose_x = matmul_10_transpose_x_0, transpose_y = matmul_10_transpose_y_0, x = transpose_68, y = transpose_69)[name = tensor<string, []>("matmul_10_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_10_cast_fp16 = add(x = matmul_10_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_10_cast_fp16")];
            tensor<int32, []> softmax_10_axis_0 = const()[name = tensor<string, []>("softmax_10_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_10_cast_fp16 = softmax(axis = softmax_10_axis_0, x = add_10_cast_fp16)[name = tensor<string, []>("softmax_10_cast_fp16")];
            tensor<bool, []> attn_output_41_transpose_x_0 = const()[name = tensor<string, []>("attn_output_41_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_41_transpose_y_0 = const()[name = tensor<string, []>("attn_output_41_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_21_cast_fp16 = transpose(perm = var_804, x = x_131_cast_fp16)[name = tensor<string, []>("transpose_79")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_41_cast_fp16 = matmul(transpose_x = attn_output_41_transpose_x_0, transpose_y = attn_output_41_transpose_y_0, x = softmax_10_cast_fp16, y = value_layer_21_cast_fp16)[name = tensor<string, []>("attn_output_41_cast_fp16")];
            tensor<int32, [4]> attn_output_43_perm_0 = const()[name = tensor<string, []>("attn_output_43_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_808 = const()[name = tensor<string, []>("op_808"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_43_cast_fp16 = transpose(perm = attn_output_43_perm_0, x = attn_output_41_cast_fp16)[name = tensor<string, []>("transpose_76")];
            tensor<fp16, [1, 512, 384]> input_167_cast_fp16 = reshape(shape = var_808, x = attn_output_43_cast_fp16)[name = tensor<string, []>("input_167_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_10_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(60224512)))];
            tensor<fp16, [384]> model_encoder_layer_10_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(60519488)))];
            tensor<fp16, [1, 512, 384]> linear_63_cast_fp16 = linear(bias = model_encoder_layer_10_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_10_attention_output_dense_weight_to_fp16, x = input_167_cast_fp16)[name = tensor<string, []>("linear_63_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_171_cast_fp16 = add(x = linear_63_cast_fp16, y = hidden_states_61_cast_fp16)[name = tensor<string, []>("input_171_cast_fp16")];
            tensor<int32, [1]> input_173_axes_0 = const()[name = tensor<string, []>("input_173_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_10_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(60520320)))];
            tensor<fp16, [384]> model_encoder_layer_10_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(60521152)))];
            tensor<fp16, [1, 512, 384]> input_173_cast_fp16 = layer_norm(axes = input_173_axes_0, beta = model_encoder_layer_10_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_10_attention_output_LayerNorm_weight_to_fp16, x = input_171_cast_fp16)[name = tensor<string, []>("input_173_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_10_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(60521984)))];
            tensor<fp16, [1536]> model_encoder_layer_10_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61701696)))];
            tensor<fp16, [1, 512, 1536]> linear_64_cast_fp16 = linear(bias = model_encoder_layer_10_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_10_intermediate_dense_weight_to_fp16, x = input_173_cast_fp16)[name = tensor<string, []>("linear_64_cast_fp16")];
            tensor<string, []> input_177_mode_0 = const()[name = tensor<string, []>("input_177_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_177_cast_fp16 = gelu(mode = input_177_mode_0, x = linear_64_cast_fp16)[name = tensor<string, []>("input_177_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_10_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(61704832)))];
            tensor<fp16, [384]> model_encoder_layer_10_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(62884544)))];
            tensor<fp16, [1, 512, 384]> linear_65_cast_fp16 = linear(bias = model_encoder_layer_10_output_dense_bias_to_fp16, weight = model_encoder_layer_10_output_dense_weight_to_fp16, x = input_177_cast_fp16)[name = tensor<string, []>("linear_65_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_181_cast_fp16 = add(x = linear_65_cast_fp16, y = input_173_cast_fp16)[name = tensor<string, []>("input_181_cast_fp16")];
            tensor<int32, [1]> hidden_states_67_axes_0 = const()[name = tensor<string, []>("hidden_states_67_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_10_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(62885376)))];
            tensor<fp16, [384]> model_encoder_layer_10_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_10_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(62886208)))];
            tensor<fp16, [1, 512, 384]> hidden_states_67_cast_fp16 = layer_norm(axes = hidden_states_67_axes_0, beta = model_encoder_layer_10_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_10_output_LayerNorm_weight_to_fp16, x = input_181_cast_fp16)[name = tensor<string, []>("hidden_states_67_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_11_attention_self_query_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_self_query_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(62887040)))];
            tensor<fp16, [384]> model_encoder_layer_11_attention_self_query_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_self_query_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63182016)))];
            tensor<fp16, [1, 512, 384]> linear_66_cast_fp16 = linear(bias = model_encoder_layer_11_attention_self_query_bias_to_fp16, weight = model_encoder_layer_11_attention_self_query_weight_to_fp16, x = hidden_states_67_cast_fp16)[name = tensor<string, []>("linear_66_cast_fp16")];
            tensor<int32, [4]> var_852 = const()[name = tensor<string, []>("op_852"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_135_cast_fp16 = reshape(shape = var_852, x = linear_66_cast_fp16)[name = tensor<string, []>("x_135_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_11_attention_self_key_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_self_key_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63182848)))];
            tensor<fp16, [384]> model_encoder_layer_11_attention_self_key_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_self_key_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63477824)))];
            tensor<fp16, [1, 512, 384]> linear_67_cast_fp16 = linear(bias = model_encoder_layer_11_attention_self_key_bias_to_fp16, weight = model_encoder_layer_11_attention_self_key_weight_to_fp16, x = hidden_states_67_cast_fp16)[name = tensor<string, []>("linear_67_cast_fp16")];
            tensor<int32, [4]> var_861 = const()[name = tensor<string, []>("op_861"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_139_cast_fp16 = reshape(shape = var_861, x = linear_67_cast_fp16)[name = tensor<string, []>("x_139_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_11_attention_self_value_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_self_value_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63478656)))];
            tensor<fp16, [384]> model_encoder_layer_11_attention_self_value_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_self_value_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63773632)))];
            tensor<fp16, [1, 512, 384]> linear_68_cast_fp16 = linear(bias = model_encoder_layer_11_attention_self_value_bias_to_fp16, weight = model_encoder_layer_11_attention_self_value_weight_to_fp16, x = hidden_states_67_cast_fp16)[name = tensor<string, []>("linear_68_cast_fp16")];
            tensor<int32, [4]> var_870 = const()[name = tensor<string, []>("op_870"), val = tensor<int32, [4]>([1, 512, 12, 32])];
            tensor<fp16, [1, 512, 12, 32]> x_cast_fp16 = reshape(shape = var_870, x = linear_68_cast_fp16)[name = tensor<string, []>("x_cast_fp16")];
            tensor<int32, [4]> var_872 = const()[name = tensor<string, []>("op_872"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<fp16, []> mul_11_y_0_to_fp16 = const()[name = tensor<string, []>("mul_11_y_0_to_fp16"), val = tensor<fp16, []>(0x1.6ap-3)];
            tensor<fp16, [1, 512, 12, 32]> mul_11_cast_fp16 = mul(x = x_135_cast_fp16, y = mul_11_y_0_to_fp16)[name = tensor<string, []>("mul_11_cast_fp16")];
            tensor<bool, []> matmul_11_transpose_y_0 = const()[name = tensor<string, []>("matmul_11_transpose_y_0"), val = tensor<bool, []>(true)];
            tensor<bool, []> matmul_11_transpose_x_0 = const()[name = tensor<string, []>("matmul_11_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<int32, [4]> transpose_70_perm_0 = const()[name = tensor<string, []>("transpose_70_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<int32, [4]> transpose_71_perm_0 = const()[name = tensor<string, []>("transpose_71_perm_0"), val = tensor<int32, [4]>([0, 2, -3, -1])];
            tensor<fp16, [1, 12, 512, 32]> transpose_71 = transpose(perm = transpose_71_perm_0, x = x_139_cast_fp16)[name = tensor<string, []>("transpose_73")];
            tensor<fp16, [1, 12, 512, 32]> transpose_70 = transpose(perm = transpose_70_perm_0, x = mul_11_cast_fp16)[name = tensor<string, []>("transpose_74")];
            tensor<fp16, [1, 12, 512, 512]> matmul_11_cast_fp16 = matmul(transpose_x = matmul_11_transpose_x_0, transpose_y = matmul_11_transpose_y_0, x = transpose_70, y = transpose_71)[name = tensor<string, []>("matmul_11_cast_fp16")];
            tensor<fp16, [1, 12, 512, 512]> add_11_cast_fp16 = add(x = matmul_11_cast_fp16, y = attention_mask_cast_fp16)[name = tensor<string, []>("add_11_cast_fp16")];
            tensor<int32, []> softmax_11_axis_0 = const()[name = tensor<string, []>("softmax_11_axis_0"), val = tensor<int32, []>(-1)];
            tensor<fp16, [1, 12, 512, 512]> softmax_11_cast_fp16 = softmax(axis = softmax_11_axis_0, x = add_11_cast_fp16)[name = tensor<string, []>("softmax_11_cast_fp16")];
            tensor<bool, []> attn_output_45_transpose_x_0 = const()[name = tensor<string, []>("attn_output_45_transpose_x_0"), val = tensor<bool, []>(false)];
            tensor<bool, []> attn_output_45_transpose_y_0 = const()[name = tensor<string, []>("attn_output_45_transpose_y_0"), val = tensor<bool, []>(false)];
            tensor<fp16, [1, 12, 512, 32]> value_layer_cast_fp16 = transpose(perm = var_872, x = x_cast_fp16)[name = tensor<string, []>("transpose_75")];
            tensor<fp16, [1, 12, 512, 32]> attn_output_45_cast_fp16 = matmul(transpose_x = attn_output_45_transpose_x_0, transpose_y = attn_output_45_transpose_y_0, x = softmax_11_cast_fp16, y = value_layer_cast_fp16)[name = tensor<string, []>("attn_output_45_cast_fp16")];
            tensor<int32, [4]> attn_output_perm_0 = const()[name = tensor<string, []>("attn_output_perm_0"), val = tensor<int32, [4]>([0, 2, 1, 3])];
            tensor<int32, [3]> var_876 = const()[name = tensor<string, []>("op_876"), val = tensor<int32, [3]>([1, 512, 384])];
            tensor<fp16, [1, 512, 12, 32]> attn_output_cast_fp16 = transpose(perm = attn_output_perm_0, x = attn_output_45_cast_fp16)[name = tensor<string, []>("transpose_72")];
            tensor<fp16, [1, 512, 384]> input_183_cast_fp16 = reshape(shape = var_876, x = attn_output_cast_fp16)[name = tensor<string, []>("input_183_cast_fp16")];
            tensor<fp16, [384, 384]> model_encoder_layer_11_attention_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(63774464)))];
            tensor<fp16, [384]> model_encoder_layer_11_attention_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64069440)))];
            tensor<fp16, [1, 512, 384]> linear_69_cast_fp16 = linear(bias = model_encoder_layer_11_attention_output_dense_bias_to_fp16, weight = model_encoder_layer_11_attention_output_dense_weight_to_fp16, x = input_183_cast_fp16)[name = tensor<string, []>("linear_69_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_187_cast_fp16 = add(x = linear_69_cast_fp16, y = hidden_states_67_cast_fp16)[name = tensor<string, []>("input_187_cast_fp16")];
            tensor<int32, [1]> input_189_axes_0 = const()[name = tensor<string, []>("input_189_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_11_attention_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64070272)))];
            tensor<fp16, [384]> model_encoder_layer_11_attention_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_attention_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64071104)))];
            tensor<fp16, [1, 512, 384]> input_189_cast_fp16 = layer_norm(axes = input_189_axes_0, beta = model_encoder_layer_11_attention_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_11_attention_output_LayerNorm_weight_to_fp16, x = input_187_cast_fp16)[name = tensor<string, []>("input_189_cast_fp16")];
            tensor<fp16, [1536, 384]> model_encoder_layer_11_intermediate_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_intermediate_dense_weight_to_fp16"), val = tensor<fp16, [1536, 384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(64071936)))];
            tensor<fp16, [1536]> model_encoder_layer_11_intermediate_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_intermediate_dense_bias_to_fp16"), val = tensor<fp16, [1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(65251648)))];
            tensor<fp16, [1, 512, 1536]> linear_70_cast_fp16 = linear(bias = model_encoder_layer_11_intermediate_dense_bias_to_fp16, weight = model_encoder_layer_11_intermediate_dense_weight_to_fp16, x = input_189_cast_fp16)[name = tensor<string, []>("linear_70_cast_fp16")];
            tensor<string, []> input_193_mode_0 = const()[name = tensor<string, []>("input_193_mode_0"), val = tensor<string, []>("EXACT")];
            tensor<fp16, [1, 512, 1536]> input_193_cast_fp16 = gelu(mode = input_193_mode_0, x = linear_70_cast_fp16)[name = tensor<string, []>("input_193_cast_fp16")];
            tensor<fp16, [384, 1536]> model_encoder_layer_11_output_dense_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_output_dense_weight_to_fp16"), val = tensor<fp16, [384, 1536]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(65254784)))];
            tensor<fp16, [384]> model_encoder_layer_11_output_dense_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_output_dense_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66434496)))];
            tensor<fp16, [1, 512, 384]> linear_71_cast_fp16 = linear(bias = model_encoder_layer_11_output_dense_bias_to_fp16, weight = model_encoder_layer_11_output_dense_weight_to_fp16, x = input_193_cast_fp16)[name = tensor<string, []>("linear_71_cast_fp16")];
            tensor<fp16, [1, 512, 384]> input_197_cast_fp16 = add(x = linear_71_cast_fp16, y = input_189_cast_fp16)[name = tensor<string, []>("input_197_cast_fp16")];
            tensor<int32, [1]> hidden_states_axes_0 = const()[name = tensor<string, []>("hidden_states_axes_0"), val = tensor<int32, [1]>([-1])];
            tensor<fp16, [384]> model_encoder_layer_11_output_LayerNorm_weight_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_output_LayerNorm_weight_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66435328)))];
            tensor<fp16, [384]> model_encoder_layer_11_output_LayerNorm_bias_to_fp16 = const()[name = tensor<string, []>("model_encoder_layer_11_output_LayerNorm_bias_to_fp16"), val = tensor<fp16, [384]>(BLOBFILE(path = tensor<string, []>("@model_path/weights/weight.bin"), offset = tensor<uint64, []>(66436160)))];
            tensor<fp16, [1, 512, 384]> hidden_states_cast_fp16 = layer_norm(axes = hidden_states_axes_0, beta = model_encoder_layer_11_output_LayerNorm_bias_to_fp16, epsilon = var_21_to_fp16, gamma = model_encoder_layer_11_output_LayerNorm_weight_to_fp16, x = input_197_cast_fp16)[name = tensor<string, []>("hidden_states_cast_fp16")];
            tensor<string, []> hidden_states_cast_fp16_to_fp32_dtype_0 = const()[name = tensor<string, []>("hidden_states_cast_fp16_to_fp32_dtype_0"), val = tensor<string, []>("fp32")];
            tensor<fp32, [1, 512, 384]> last_hidden_state = cast(dtype = hidden_states_cast_fp16_to_fp32_dtype_0, x = hidden_states_cast_fp16)[name = tensor<string, []>("cast_53")];
        } -> (last_hidden_state);
}