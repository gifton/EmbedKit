import Foundation
import VectorCore

/// Protocol defining the interface for an embedding model.
/// Models must be actors to ensure thread safety during inference.
public protocol EmbeddingModel: Actor {
    /// The unique identifier for this model instance
    var id: String { get }
    
    /// The dimensionality of the embeddings generated by this model
    var dimension: Int { get }
    
    /// Generates an embedding for a single string of text
    /// - Parameter text: The text to embed
    /// - Returns: An Embedding struct containing the vector and metadata
    func embed(_ text: String) async throws -> Embedding
    
    /// Generates embeddings for a batch of texts
    /// - Parameter texts: The texts to embed
    /// - Returns: A BatchResult containing embeddings and metrics
    func embedBatch(_ texts: [String]) async throws -> BatchResult
}

/// Protocol defining the interface for a tokenizer.
public protocol Tokenizer: Sendable {
    /// Encodes text into a sequence of token IDs
    /// - Parameter text: The text to tokenize
    /// - Returns: An array of token IDs
    func encode(_ text: String) async throws -> [Int]
    
    /// Decodes a sequence of token IDs back into text
    /// - Parameter tokens: The token IDs to decode
    /// - Returns: The reconstructed string
    func decode(_ tokens: [Int]) async throws -> String
}

/// Protocol for model backends (CoreML, Metal, etc.)
public protocol ModelBackend: Sendable {
    /// The type of device this backend uses
    var device: ComputeDevice { get }
    
    /// Current memory usage in bytes
    var memoryUsage: Int64 { get }
}

public enum ComputeDevice: String, Sendable {
    case cpu
    case gpu
    case neuralEngine
}
